{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| x1  | x2  | x3  | y |\n",
    "| --- | --- | --- | - |\n",
    "| 001 | 010 | 100 | 1 |\n",
    "| 100 | 111 | 000 | 0 |\n",
    "| 001 | 010 | 100 | 1 |\n",
    "| 100 | 000 | 111 | 1 |\n",
    "| 111 | 010 | 100 | 0 |\n",
    "| 100 | 010 | 000 | 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x1,x2,x3就是$\\color{#FF0000}{seq len}$序列的长度，每个下面由一个长度为3的向量组成，这个向量就是$\\color{#FF0000}{input size}$。  \n",
    "对于语句一行数据就是一句话，每句话有三个单词组成，每个单词用001这样的one-hot编码。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](image/rnn.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](image/rnn_nlp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](image/rnn2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面图可以看出torch.ones(seq_len,batch_size,input_size)，这样一个输入数据第一个维度是序列 也就是I,hate,this这三  \n",
    "batch_size相当于该序列位置上所有批次数据，input_size是该序列特征维度。  \n",
    "[batch_size,input_size] * [input_size,out_size] 就是X*W的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立输入到输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "out输出的是所有h1到h_N的隐层结果，hidden输出的就是最后一次计算后的结果值。这里N个RNN层其实就是同一个"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_size是输入多少条数据  \n",
    "seq_len是循环的长度，比如根据前n天天气的n，前n天股票信息的n  \n",
    "input_size是每一天的信息维度  \n",
    "hidden_size是神经元数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch_first = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_size = 12\n",
    "        self.hidden_size = 64\n",
    "        self.num_layers = 2\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # 初始的隐层输入，可以不填\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, hidden = self.rnn(x,h0)\n",
    "        return out,hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 28, 64]), torch.Size([2, 128, 64]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "# 输入维度[batch,seq_len,feature]\n",
    "x = torch.rand(128,28,12)\n",
    "output,hidden = model(x)\n",
    "output.shape,hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True的模式下  \n",
    "output维度是[batch,seq_len,hidden_size]  \n",
    "hidden的维度是[num_layers,batch,hidden_size]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch_first = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_size = 12\n",
    "        self.hidden_size = 64\n",
    "        self.num_layers = 2\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=False,\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # 初始的隐层输入，可以不填\n",
    "        h0 = torch.zeros(self.num_layers, x.size(1), self.hidden_size)\n",
    "        out, hidden = self.rnn(x,h0)\n",
    "        return out,hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([28, 128, 64]), torch.Size([2, 128, 64]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net2()\n",
    "# 输入维度[seq_len,batch,eature]\n",
    "x = torch.rand(28,128,12)\n",
    "output,hidden = model(x)\n",
    "output.shape,hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False的模式下  \n",
    "output维度是[seq_len,batch,hidden_size]  \n",
    "hidden的维度是[num_layers,batch,hidden_size] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1120, -0.1544, -0.1922,  ..., -0.1320,  0.0459,  0.1618],\n",
       "        [ 0.1295, -0.1396, -0.2902,  ..., -0.0764,  0.0439,  0.1843],\n",
       "        [ 0.2361, -0.2360, -0.3124,  ..., -0.1758, -0.0555,  0.2236],\n",
       "        ...,\n",
       "        [ 0.1080, -0.2091, -0.2772,  ..., -0.3361, -0.1193,  0.2419],\n",
       "        [ 0.0247, -0.1583, -0.3389,  ..., -0.2296, -0.0656,  0.1738],\n",
       "        [ 0.1520, -0.1978, -0.4128,  ..., -0.1930, -0.1602,  0.2644]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[-1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1120, -0.1544, -0.1922,  ..., -0.1320,  0.0459,  0.1618],\n",
       "        [ 0.1295, -0.1396, -0.2902,  ..., -0.0764,  0.0439,  0.1843],\n",
       "        [ 0.2361, -0.2360, -0.3124,  ..., -0.1758, -0.0555,  0.2236],\n",
       "        ...,\n",
       "        [ 0.1080, -0.2091, -0.2772,  ..., -0.3361, -0.1193,  0.2419],\n",
       "        [ 0.0247, -0.1583, -0.3389,  ..., -0.2296, -0.0656,  0.1738],\n",
       "        [ 0.1520, -0.1978, -0.4128,  ..., -0.1930, -0.1602,  0.2644]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden[-1,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size 输入特征的大小\n",
    "# hidden_size 神经元模块额数量\n",
    "# num_layer 几层隐藏层\n",
    "# lstm默认输入的维度是 (seq_len,batch,feature)\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self,flag):\n",
    "        super().__init__()\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size = 12,\n",
    "            hidden_size = 64,\n",
    "            num_layers = 1,\n",
    "            batch_first = flag\n",
    "        )\n",
    "        self.out = torch.nn.Linear(in_features = 64,out_features = 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # output包含每个序列的输出结果\n",
    "        # h_n 只包含最后一个序列的输出\n",
    "        # c_n 只包含最后一个序列的输出\n",
    "        # h是最终输出，c是模块里cell的输出 \n",
    "        output,(h_n,c_n) = self.lstm(x)\n",
    "        return output,(h_n,c_n)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 28, 64]), torch.Size([1, 128, 64]), torch.Size([1, 128, 64]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_first设置为True\n",
    "model = LSTM(True)\n",
    "# batch,seq_len,feature\n",
    "x = torch.rand(128,28,12)\n",
    "a,(b,c) = model(x)\n",
    "a.shape,b.shape,c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True的模式下  \n",
    "output维度是[batch,seq_len,hidden_size]  \n",
    "h_n的维度是[num_layers,batch,hidden_size]  \n",
    "c_n的维度是[num_layers,batch,hidden_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([28, 128, 64]), torch.Size([1, 128, 64]), torch.Size([1, 128, 64]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_first设置为False\n",
    "model = LSTM(False)\n",
    "# seq_len,batch,feature\n",
    "x = torch.rand(28,128,12)\n",
    "a,(b,c) = model(x)\n",
    "a.shape,b.shape,c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False的模式下  \n",
    "output维度是[seq_len,batch,hidden_size]  \n",
    "h_n的维度是[num_layers,batch,hidden_size]  \n",
    "c_n的维度是[num_layers,batch,hidden_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1301,  0.0610, -0.0604,  ..., -0.0548,  0.0606,  0.2497],\n",
       "        [ 0.1255,  0.0589, -0.0490,  ..., -0.0105,  0.0410,  0.2842],\n",
       "        [ 0.1395,  0.0280, -0.0617,  ..., -0.0073,  0.0297,  0.2154],\n",
       "        ...,\n",
       "        [ 0.1219,  0.0582, -0.0552,  ..., -0.0449,  0.0457,  0.2289],\n",
       "        [ 0.1080,  0.0659, -0.0380,  ..., -0.0206,  0.0564,  0.2877],\n",
       "        [ 0.0973,  0.0531, -0.0474,  ..., -0.0218,  0.0481,  0.2067]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[-1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1301,  0.0610, -0.0604,  ..., -0.0548,  0.0606,  0.2497],\n",
       "         [ 0.1255,  0.0589, -0.0490,  ..., -0.0105,  0.0410,  0.2842],\n",
       "         [ 0.1395,  0.0280, -0.0617,  ..., -0.0073,  0.0297,  0.2154],\n",
       "         ...,\n",
       "         [ 0.1219,  0.0582, -0.0552,  ..., -0.0449,  0.0457,  0.2289],\n",
       "         [ 0.1080,  0.0659, -0.0380,  ..., -0.0206,  0.0564,  0.2877],\n",
       "         [ 0.0973,  0.0531, -0.0474,  ..., -0.0218,  0.0481,  0.2067]]],\n",
       "       grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现output的最后一个数就等于h_n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
