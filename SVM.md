# 线性可分（硬间隔）

优化目标
$$
min:\left \| W \right \|^{2}\\
st:y_{i}[w^{T}x_{i}+b]\geq 1
$$
公理1：点到直线距离，w1w2是参数
$$
w_{1}x+w_{2}y+ b = 0\\
d =\frac{|w_{1}x_{0}+w_{2}y_{0}+b|}{\sqrt{w_{1}^{2}+w_{2}^{2}}}
$$
公理2：点到超平面的距离
$$
W^{T}X+ b = 0\\
d =\frac{|W^{T}X_{0}+b|}{\left \| W \right \|}
$$
第一个是直线或平面方程，第二个表示某个点到上述平面方程的距离。因为平面方程可以乘以个任意一个数而此平面仍然是原来那一个，所以我们可以让距离平面最近的点到平面的距离d中的分子为1(这也就是支撑向量的由来)。svm的优化目标就是让支撑向量点到平面距离d最大。

<font color="red">最后使用时候不需要求得w的实际解，只需要把待预测的x代入计算大于小于0来进行分类判断就行了，也因此这里我们缩放w,b没有任何问题</font>

# 非线性问题（软间隔）

$$
min:\left \| W \right \|^{2} + C\sum \xi_{i}\\
st:y_{i}[w^{T}x_{i}+b]\geq 1-\xi_{i}\\
st:\xi_i\geq0
$$

上式中最右边那个叫松弛变量它是一个未知变量，小标i表示每个数据样本都有一个自己的松弛变量。

松弛变量=0，表示点在支撑向量上

0<松弛变量<1，点在支持向量内，相当于增加了泛化能力让约束条件不那么严格，否者按照硬间隔的话超平面会变得很窄

松弛变量>1，点就分错了，分到超平面另一边去了

C就是正则项，C越大那么就会驱使松弛变量更小，那么就会更拟合，反之C越小松弛变量可以更大那么能更泛化

![](img\svm.jpg)

# 核函数

$$
min:\left \| W \right \|^{2} + C\sum \xi_{i}\\
st:y_{i}[w^{T}\varphi(x_i)+b]\geq 1-\xi_{i}\\
st:\xi_i\geq0
$$

唯一区别是给x套上一个映射函数，从低维映射到高维。但是我们不需要知道这个映射函数的具体形式，只需要知道kernal函数即可，等式右边是两个映射后函数向量相乘后结果就是一个数
$$
K(x_1,x_2) = \varphi(x_1)^T\varphi(x_2)
$$


# 拉格朗日

$$
min:f(x) = x_{1} + x_{2}\\
st:h(x) = x_{1}^2 + x_{2}^2 - 2 = 0
$$

构造拉格朗日函数：
$$
L(x,\mu) = (x_{1} + x_{2}) + \mu (x_{1}^2 + x_{2}^2 - 2 )
$$
然后分别对x1,x2,\mu求偏导得到3个等式，三个未知数三个方程最后就能得到结果了
$$
1 + 2*\mu*x_{1} = 0\\
1 + 2*\mu*x_{2} = 0\\
x_{1}^2 + x_{2}^2 - 2 = 0
$$
