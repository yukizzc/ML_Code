{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eb94db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21481f20",
   "metadata": {},
   "source": [
    "# 均方误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3e0e8760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个预测值和实际值的损失值：0.0\n",
      "第2个预测值和实际值的损失值：1.0\n",
      "第3个预测值和实际值的损失值：4.0\n"
     ]
    }
   ],
   "source": [
    "y = torch.tensor([1., 1., 1., 1.])\n",
    "y_pre1 = torch.tensor([1, 1, 1, 1])\n",
    "y_pre2 = torch.tensor([2, 2, 2, 2])\n",
    "y_pre3 = torch.tensor([3, 3, 3, 3])\n",
    "criterion = nn.MSELoss()\n",
    "y_pre_li = [y_pre1, y_pre2, y_pre3]\n",
    "for i in range(len(y_pre_li)):\n",
    "    # 预测值必须只有一个维度，用这个函数扁平化values.squeeze(1)\n",
    "    output = criterion(y_pre_li[i], y)\n",
    "    print('第{}个预测值和实际值的损失值：{}'.format(i+1, output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1dda045c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 算法原理\n",
    "torch.mean((y - y_pre3)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ba0f31",
   "metadata": {},
   "source": [
    "# 二分类交叉熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "689c0e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个预测值和实际值的损失值：0.5001052618026733\n",
      "第1个预测值和实际值的损失值：1.3380919694900513\n",
      "第2个预测值和实际值的损失值：0.7240769267082214\n"
     ]
    }
   ],
   "source": [
    "# BCELoss, 预测值经过sigmoid后转换成0~1之间，标签则是0或者1\n",
    "y = torch.Tensor([0,0,0,0,0,1,1,1,1,1])\n",
    "y_pre1 = torch.Tensor([0.1,0.1,0.2,0.2,0.8,1.5,1.2,2.2,3.8,1.8])\n",
    "y_pre2= torch.Tensor([2.2,2.2,2.2,2.2,2.2,0.8,0.8,0.8,0.8,0.8])\n",
    "y_pre3= torch.Tensor([0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5])\n",
    "# 预测值要加一层sigmoid的输出\n",
    "y_pre_li = [nn.Sigmoid()(y_pre1),nn.Sigmoid()(y_pre2),nn.Sigmoid()(y_pre3)]\n",
    "criterion = nn.BCELoss(reduction='mean')\n",
    "for i in range(len(y_pre_li)):\n",
    "    output = criterion(y_pre_li[i],y)\n",
    "    print('第{}个预测值和实际值的损失值：{}'.format(i, output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d257d71",
   "metadata": {},
   "source": [
    "$\\frac{-1}{n}\\sum_{n}^{i}(y[i]*log(y_{pre}[i]))+(1-y[i])*log(1-y_{pre}[i])$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7eb7b40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5001)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 算法\n",
    "output = -1*torch.mean(y*torch.log(nn.Sigmoid()(y_pre1)) + (1-y)*(torch.log(1-nn.Sigmoid()(y_pre1))))\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beade041",
   "metadata": {},
   "source": [
    "# 多分类交叉熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9ca53564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个预测值和实际值的损失值：0.612541675567627\n",
      "第2个预测值和实际值的损失值：2.7241179943084717\n"
     ]
    }
   ],
   "source": [
    "# 标签是一维的，有几种不同数字就是分类数目\n",
    "y = torch.LongTensor([0, 1, 2, 0])\n",
    "# 行数就是数据个数，列数就是分类数目\n",
    "y_pre1 = torch.tensor([[1, 0.2, 0.1],\n",
    "                    [0.1, 1, 0.1],\n",
    "                    [0.1, 0.2, 1],\n",
    "                    [1, 0.2, 0.1]])\n",
    "\n",
    "y_pre2 = torch.tensor([[1, 0.2, 5],\n",
    "                    [0.1, 1, 5],\n",
    "                    [3, 0.2, 1],\n",
    "                    [0.9, 0.2, 0.1]])              \n",
    "# 每一个数据做softmax处理\n",
    "m = nn.LogSoftmax(dim=1)\n",
    "y_pre_li = [m(y_pre1), m(y_pre2)]\n",
    "# nn.NLLLoss的输入target是类别值\n",
    "criterion = nn.NLLLoss()\n",
    "for i in range(2):\n",
    "    output = criterion(y_pre_li[i], y)\n",
    "    print('第{}个预测值和实际值的损失值：{}'.format(i+1, output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9d1fa560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6184, -1.4184, -1.5184],\n",
       "        [-1.4951, -0.5951, -1.4951],\n",
       "        [-1.5184, -1.4184, -0.6184],\n",
       "        [-0.6184, -1.4184, -1.5184]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(y_pre1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201a8eb5",
   "metadata": {},
   "source": [
    "# 多分类第二种方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "60e7f30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个预测值和实际值的损失值：0.612541675567627\n",
      "第2个预测值和实际值的损失值：2.7241179943084717\n"
     ]
    }
   ],
   "source": [
    "# 标签，这里注意必须是LongTensor类型\n",
    "y = torch.LongTensor([0, 1, 2, 0])\n",
    "# CrossEntropyLoss多分类，用这种方便,这里4行代表4个数据, 3列代表3种分类\n",
    "y_pre1 = torch.tensor([[1, 0.2, 0.1],\n",
    "                    [0.1, 1, 0.1],\n",
    "                    [0.1, 0.2, 1],\n",
    "                    [1, 0.2, 0.1]])\n",
    "\n",
    "y_pre2 = torch.tensor([[1, 0.2, 5],\n",
    "                    [0.1, 1, 5],\n",
    "                    [3, 0.2, 1],\n",
    "                    [0.9, 0.2, 0.1]]) \n",
    "y_pre_li = [y_pre1, y_pre2]\n",
    "# 传入预测值的维度就是类别数量（几种分类），第二个是真实值的维度就是一维用的是类别的索引（0表示第一个类别，2表示第三个类别）\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for i in range(2):\n",
    "    output = criterion(y_pre_li[i], y)\n",
    "    print('第{}个预测值和实际值的损失值：{}'.format(i+1, output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6916e3d6",
   "metadata": {},
   "source": [
    "<font color=#A52A2A size=4 >nn.NLLLoss的输入target是类别值，并不是one-hot编码格式，这个要注意！！  \n",
    "CrossEntropyLoss()的target输入也是类别值，不是one-hot编码格式</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217e216e",
   "metadata": {},
   "source": [
    "$loss(x,class) = -log(\\frac{exp^{x[class]}}{\\sum exp^{x[i]}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2bea2215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6125417351722717"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 算法实现\n",
    "# 定义softmax函数\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "# 利用numpy计算\n",
    "def cross_entropy_np(x, y):\n",
    "    x_softmax = [softmax(x[i]) for i in range(len(x))]\n",
    "    x_log = [np.log(x_softmax[i][y[i]]) for i in range(len(y))]\n",
    "    loss = - np.sum(x_log) / len(y)\n",
    "    return loss\n",
    "cross_entropy_np(y_pre1.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ef8fb4",
   "metadata": {},
   "source": [
    "BCELoss是Binary CrossEntropyLoss的缩写，BCELoss是CrossEntropyLoss的一个特例，只用于二分类问题，而CrossEntropyLoss可以用于二分类，也可以用于多分类。  \n",
    "使用nn.BCELoss需要在该层前面加上Sigmoid函数。  \n",
    "使用nn.CrossEntropyLoss会自动加上Softmax层。  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
